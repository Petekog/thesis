{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MANP problem (<u>M</u>aximizing <u>A</u>verage <u>N</u>umber of <u>P</u>ages read in multi-page publications) as a reverse MDP}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will present MANP problem as MDP. In order to simplify the representation at this stage we add two constraints on the initial model:\n",
    "1. On each step the agent has only two possible actions - \"next\" and \"leave\".\n",
    "+ The agent can't visit the same page twice.\n",
    "\n",
    "The problem remains the same: given a set of content units (pages)  $B = \\{c_1\\dots c_K\\}$, we want to present it to users in such a way that will maximize the average number of pages viewed. The only thing we can change is an order of the representation for each particular user.<br>\n",
    "\n",
    "Even though the naive and straightforward way to define the MDP is to set the content consumer being an agent, we will do it in a different way. The content producer will be an agent and the Web resource visitor will be the part of the environment. Such a formulation allows us to use classic tools of planning to optimize the target MANP problem's value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr style=\"border:2px solid gray\"> </hr>\n",
    "\n",
    "**Here is a definition of the MDP:**<br>\n",
    "\n",
    " 1. State space $S = S' \\cup \\{s_{init},s_{final}\\}$ : \n",
    "    * There will be an initial state $s_{init}$. The visitor haven't seen yet any content.\n",
    "    * Each state $s_j = C_k \\in S'$ will contain all the pages the user saw $C_k \\subset B$ (nodes the user visited).\n",
    "    * There will be a final state $s_{final}$. It represents the fact that the user finished watching the pages.\n",
    " <br> <br> \n",
    " +  Action space $A=\\{a_{c_1}\\dots a_{c_k}\\}$. Action $a_{c_k}$ denotes the action of presenting  to the visitor page $c_k \\in B$.\n",
    " <br><br>\n",
    " +  Reward function $R(a,s,s')$ for $a \\in A\\;s,s'\\in S$  depends only on the resulting state $s'$ :\n",
    "     * If the agent traversed to $s_{final}$, the reward is zero. Otherwise it's 1.<br>\n",
    "        $\\forall a\\in A\\;s_i\\in S :R(a,s_i,s_{final}) = 0$, otherwise $R(a,s,s')=1$\n",
    " <br><br>\n",
    " + Transition function $T(a,s,s')$ depends only on action $a$ and it is not known to us. We will denote it as $T'(a) = T(a,s,s')$<br>\n",
    "   It actually determines possible reactions of the content consumer to the page $c_i$ (or for action $a_{c_i}$ in terms of MDP) , the probability that the user press \"next\" or \"leave\". <br>\n",
    "   At the next stages we may assume that it depends also on $s$ ,i.e. visitors behavior depends on the pages seen.\n",
    "<hr style=\"border:2px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition probabilities are known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first imagine the situation when the preferences of the visitor are known, and they are encapsulated in the probability of leaving for every particular page. It completely defines the transition function and the MDP . In this case the  problem is reduced to the planning problem.<br> \n",
    "These four characteristics made the current planning problem's solution trivial.\n",
    "1. The transition function is independent of the pages the user has visited, and depend only on the current page shown. \n",
    "+ The states can't be repeated for each particular user, the user can see each page only once.\n",
    "+ The user can traverse to exactly two states given some action,and one of them is $s_{final}$.\n",
    "+ The reward depends only on the forthcoming state, and it is equal for all states except for the final.\n",
    "\n",
    "Let's consider the state value function given any policy $\\pi$.<br>\n",
    "&emsp;$V(s_i) = \\sum_{j}T(\\pi(s_i),s_i,s_j)( R(\\pi(s_i),s_i,s_j) + V(s_j) )$<br>\n",
    "There is only one possible descending state with non-zero value for each action (The provider presents content to the visitor and she proceeds to the next page) and the reward is 1. As a result we can simplify the value function for our MDP:<br>\n",
    "&emsp;$V(s_i) = T(\\pi(s_i),s_i,s_j)( 1 + V(s_j) )$ ,<br> If we substitute the previous expression for each state into the value function of the initial state, we receive <br> \n",
    "&emsp;$V(s_{init}) =  T(\\pi(s_{init}),s_{init},s_i)( 1 +  T(\\pi(s_i),s_i,s_j)( 1 + \\dots ) )$.<br>\n",
    "\n",
    "Given the fact that: (1) each state can appear only once, (2) the transition probability depends only on the action, we can say that in order to maximize the value of $s_{init}$ (and as a result find an optimal policy) we need to arrange an actions in descending order by there transition probabilities. That is the optimal policy's action in the initial state will be $\\arg\\max_{\\pi(s_{init})\\in A} T(\\pi(s_{init}),s_{init},s_i)$ and so on.<br>\n",
    "<br>\n",
    "As we said before, the planning problem turns to be trivial in case we know all the data.<br>\n",
    "Now let's consider the case when our simplifying assumptions remain valid, but we don't know user's preferences.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition probabilities are unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknown content consumer's preferences mean that we don't know the exact probability of \"leaving\" for each particular page $c_i \\in B$. In terms of the MDP it means that the transition function is unknown. But in fact it is only partially true. Given an action $a_{c_i}$ (showing to the user the page $c_i$), we know exactly the states that the process has non-zero probabilities to traverse to. This is the final state and the state that contains all the pages the user visited including the last one $c_i$. So actually we need to find only the probability of leaving after seeing each page and the MDP will be fully known.<br>\n",
    "This problem turns to be a learning problem.<br>\n",
    "Again, the problem's structure together with the assumptions makes it relatively simple learning problem. Despite the fact that there are $2^{|B|} + 2$ states, only actions have an impact on the transition probability. Thus we must estimate only $|B|$ values: the probabilities of leaving for each page. <br>\n",
    "\n",
    "The simple \"model learning -> planning\" loop seems appropriate solution to deal it. But even though each step appears to be non-complicated, combining them together arise a fundamental exploration/exploitation dilemma.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
