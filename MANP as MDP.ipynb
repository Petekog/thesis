{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANP problem (*M*aximizing *A*verage *N*umber of *P*ages read in multi-page publications) as an MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:0.5px solid gray\"> </hr>\n",
    "\n",
    "### Contents:\n",
    "1. [Introduction](#intro)\n",
    "2. [Formal MDP definition](#formal)\n",
    "3. [MDP Implementation](#impl)<br>\n",
    "    a.[Basic objects](#basic)<br>\n",
    "    b.[MDP basic elements](#elem)<br>\n",
    "4. [Tests of the MDP](#tests)<br>\n",
    "    a.[Test1](#test1)<br>\n",
    "    b.[Test2](#test2)<br>\n",
    "5. [Temporary conclusion](#conclude)\n",
    "\n",
    "<hr style=\"border:0.5px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "<a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will present MANP problem as MDP. In order to simplify the representation at this stage we add two constraints on the initial model:\n",
    "1. On each step the agent has only two possible actions - \"next\" and \"leave\".\n",
    "2. The agent can't visit the same page twice.\n",
    "\n",
    "The problem remains the same: given a set of content units (pages)  $B = \\{c_1\\dots c_K\\}$, we want to present it to users in such a way that will maximize the average number of pages viewed. The only thing we can change is an order of the representation for each particular user.<br>\n",
    "\n",
    "Even though the naive and straightforward way to define the MDP is to set the content consumer being an agent, we will do it in a different way. The content producer will be an agent and the Web resource visitor will be the part of the environment. Such a formulation allows us to use classic tools of planning to optimize the target MANP problem's value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formal MDP definition\n",
    "<a id='formal'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr style=\"border:2px solid gray\"> </hr>\n",
    "\n",
    "**Here is a definition of the MDP:**<br>\n",
    "\n",
    " 1. State space $S = S' \\cup \\{s_{init},s_{final}\\}$ : \n",
    "    * There will be an initial state $s_{init}$. The visitor haven't seen yet any content.\n",
    "    * Each state $s_j = C_k \\in S'$ will contain all the pages the user saw $C_k \\subset B$ (nodes the user visited).\n",
    "    * There will be a final state $s_{final}$. It represents the fact that the user finished watching the pages.\n",
    " <br> <br> \n",
    " 2.  Action space $A=\\{a_{c_1}\\dots a_{c_k}\\}$. Action $a_{c_k}$ denotes the action of presenting  to the visitor page $c_k \\in B$.\n",
    " <br><br>\n",
    " 3.  Reward function $R(a,s,s')$ for $a \\in A\\;s,s'\\in S$  depends only on the resulting state $s'$ :\n",
    "     * If the agent traversed to $s_{final}$, the reward is zero. Otherwise it's 1.<br>\n",
    "        $\\forall a\\in A\\;s_i\\in S :R(a,s_i,s_{final}) = 0$, otherwise $R(a,s,s')=1$\n",
    " <br><br>\n",
    " 4. Transition function $T(a,s,s')$ depends only on action $a$ and it is not known to us. We will denote it as $T'(a) = T(a,s,s')$<br>\n",
    "   It actually determines possible reactions of the content consumer to the page $c_i$ (or for action $a_{c_i}$ in terms of MDP) , the probability that the user press \"next\" or \"leave\". <br>\n",
    "   At the next stages we may assume that it depends also on $s$ ,i.e. visitors behavior depends on the pages seen.\n",
    "<hr style=\"border:2px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy\n",
    "For this model the policy is an objective for an optimization, the content producer needs to learn the best way to present the user the content units. The goal is not only learning the optimal policy but doing it as fast as possible, or in terms of learning - by accumulating as less regret as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MDP Implementation\n",
    "<a id='impl'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[POMDPs.jl](https://github.com/JuliaPOMDP/POMDPs.jl) package will be used to help modeling the MDP. It provides flexible interface that is compatable with different tools for solving and simulating an MDP. It specifies the set of functions that should be implemented. In fact we overload the methods of POMDPs package to work with the process we define. \n",
    "\n",
    "In following several subsections we implement this methods one by one and provide a short explanation for the  implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Packages\n",
    "<a id='basic'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "try\n",
    "    using Pkg\n",
    "    using POMDPs            # POMDPs.jl interface\n",
    "    using POMDPModelTools   # POMDPModelTools has tools that help build the MDP definition\n",
    "    using POMDPPolicies\n",
    "    using POMDPSimulators\n",
    "    using Combinatorics\n",
    "    using DiscreteValueIteration\n",
    "    using StatsPlots\n",
    "    using Plots\n",
    "\n",
    "catch  LoadError\n",
    "    Pkg.add(\"POMDPs\")\n",
    "    Pkg.add(\"POMDPModelTools\")\n",
    "    Pkg.add(\"POMDPPolicies\")\n",
    "    Pkg.add(\"POMDPSimulators\")\n",
    "    Pkg.add(\"Combinatorics\")\n",
    "    Pkg.add(\"DiscreteValueIteration\")\n",
    "    Pkg.add(\"StatsPlots\")\n",
    "    Pkg.add(\"Plots\")\n",
    "\n",
    "    using DiscreteValueIteration\n",
    "    using POMDPs\n",
    "    using POMDPModelTools\n",
    "    using POMDPPolicies\n",
    "    using POMDPSimulators\n",
    "    using Combinatorics\n",
    "    using StatsPlots\n",
    "    using Plots\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic objects\n",
    "\n",
    "Two basic objects of the model are **Page** and **User1**. \n",
    "\n",
    " <span style='background:#D3D3D3'> User1 </span> object will encapsulate the particular user's behavior.\n",
    "Although the user is a part of an environment in this model, and it may be fully described by the transition function of the process , we decided to represent it as a separate object. It was done for the several reasons: \n",
    "1. Preserving the initial structure of MANP problem , where the user is an essential part  \n",
    "2. Having a convenient ability to change the user's behavior i.e simulating the arrival of different types of users \n",
    "3. Reusing the same object in other models of MANP problem.\n",
    "\n",
    "<span style='background:#D3D3D3'> User1 </span> key attribute is <span style='background:#D3D3D3'> userPreferencesFunction </span> function. Given a pages seen so far and the current page shown her, the function returns the distribution on the possible user's reactions (leaving and proceeding).\n",
    "\n",
    "<span style='background:#D3D3D3'> Page </span> is the object that represents the unit of content which the user consumes (watches) before deciding on her next action (leaving or proceeding) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Page\n",
    "   id :: Int64 \n",
    "end\n",
    "\n",
    "pages_collection = nothing\n",
    "\n",
    "struct User1 \n",
    "   id :: Int64\n",
    "   userPreferencesFunction:: Any   # Recieve pages visited, current page shown\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     5
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "function CreatePages(number :: Int64)\n",
    "    global pages_collection = [Page(id) for id in 1:number]\n",
    "end\n",
    "\n",
    "function BitArrToInt(arr)\n",
    "    return sum(arr .* (2 .^ collect(0:length(arr)-1)))\n",
    "    end;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP basic elements\n",
    "<a id='elem'></a>\n",
    "#### States and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we define such a parts of the MDP as actions , states and the mdp object itself.\n",
    "We remind that our representation, when the user is a part of the environment and the content provider is an agent, may seem counterintuitive at first sight. But it makes sense supposing that the goal is to maximize the number of pages viewed, which is a content provider's goal.\n",
    "\n",
    "The <span style='background:#D3D3D3'> Action1</span>  object represents the action of presenting to the user a particular unit of content (aka page) .\n",
    "\n",
    "The <span style='background:#D3D3D3'> State1</span> object specifies state. It contains the pages that the user already have seen and the indicator whether it is a terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Action1\n",
    "   show_page :: Union{Page,Nothing}           # The page that is presented to the user\n",
    "end\n",
    "\n",
    "struct State1\n",
    "    visited_pages :: Any                      # Didn't specified, because we need here immutable type.                                              # Supposed to be an immutable set\n",
    "    if_terminal :: Bool\n",
    "end "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDP object\n",
    "As a part of the POMDPs interface specifications , we should define an MDP object type <span style='background:#D3D3D3'> ContentProducerMdp </span>  that inherits from the abstract type MDP parametrized with the type of actions' and states' object. The structure contains the parameters of the MDP environment for MANP. It is passed as a first argument to all the POMDPs interface's functions to navigate to the implementations that was written for this concrete MDP. \n",
    "\n",
    "It will contain a current user that determine the process dynamics, array of content units (pages) as well as tools for tracking and statistics. \n",
    "\n",
    "In addition we define <span style='background:#D3D3D3'> MdpStatistics1 </span>, the object that aggregates the statistics for the concrete <span style='background:#D3D3D3'> ContentProducerMdp </span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "mutable struct MdpStatistics1\n",
    "   current_user_path :: Array{Page,1} \n",
    "   nexts_per_page :: Array{Int64,1}\n",
    "    leaves_per_page :: Array{Int64,1}\n",
    "    total_pages_seen :: Int64\n",
    "    users_number :: Int64\n",
    "    pages_seen_per_user :: Array{Int64,1}\n",
    "    page_fialures_num :: Int64\n",
    "    \n",
    "end\n",
    "\n",
    "mutable struct ContentProducerMdp <: MDP{State1,Action1}\n",
    "    pages :: Array{Page,1}\n",
    "    current_user :: User1\n",
    "    statistics :: MdpStatistics1\n",
    "    statistics_on :: Bool            # Whether the statistics should be collected\n",
    "                                     # (Not neccessary for value iteration for example)\n",
    "    \n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auxiliary functions specified in POMDPs interface\n",
    "Return the array of all the state and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     7
    ]
   },
   "outputs": [],
   "source": [
    "function POMDPs.states(mdp::ContentProducerMdp)\n",
    "    pages_superset = collect(powerset(mdp.pages))             # We assume that the function preserves the order (not in docs)\n",
    "    states = [State1(Tuple(pages),false) for pages in pages_superset]  \n",
    "    push!(states, State1(Tuple([]),true))\n",
    "    return states\n",
    "end\n",
    "\n",
    "function POMDPs.actions(mdp::ContentProducerMdp)\n",
    "    actions = [Action1(page) for page in mdp.pages]  \n",
    "end\n",
    "\n",
    "POMDPs.isterminal(mdp::ContentProducerMdp,s::State1)= return s.if_terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition function\n",
    "\n",
    "In current less complicated formulation of the problem, the transition distribution depends only on the action. The user decision whether to proceed or to leave explained only by page that was presented him last.\n",
    "\n",
    "Exactly two states have non-zero probabilities given action $a_i$. They are: 1) the final state and 2) the state that contains all the pages the user visited including the last one $c_i$. The transition distribution on these states is defined by the probability of the user to leave for each page. The transition function queries the current user to get it (userPreferencesFunction attribute of User1). <br>\n",
    "Special case is when the visitor already saw all the pages, then the MDP is automatically traversed to the final state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.transition(mdp::ContentProducerMdp,state::State1,act::Action1)\n",
    "\n",
    "    pages_visited = state.visited_pages\n",
    "    current_page = act.show_page\n",
    "    \n",
    "    # Attemp to show the same page twice\n",
    "    if current_page in pages_visited\n",
    "        return SparseCat([State1(Tuple([]),true)], [1.0]) \n",
    "    end\n",
    "    \n",
    "    # Attemp to take action in the terminal state\n",
    "    if POMDPs.isterminal(mdp,state)\n",
    "       return SparseCat([State1(Tuple([]),true)], [1.0]) \n",
    "    end\n",
    "    \n",
    "    # All the pages have been seen by the user\n",
    "    if length(pages_visited) == length(mdp.pages) - 1\n",
    "       return SparseCat([State1(Tuple([]),true)], [1.0])\n",
    "    end\n",
    "    \n",
    "    # Getting user's leaving probability\n",
    "    leaving_probab = mdp.current_user.userPreferencesFunction(collect(Page,pages_visited),current_page)\n",
    "    \n",
    "    # Creating the next state\n",
    "    #\n",
    "    if length(pages_visited) == 0\n",
    "        new_page_visited = [current_page]\n",
    "    else\n",
    "        new_page_visited = push!(collect(pages_visited),current_page)\n",
    "    end\n",
    "    \n",
    "    # Sort the pages in the newly created state by id\n",
    "    sort!(new_page_visited,by=page->page.id,alg=InsertionSort)  # The array is almost sorted , that's why insertion sort\n",
    "    \n",
    "    # Return distribution object on possible states\n",
    "    return SparseCat([State1(Tuple(new_page_visited),false), State1(Tuple([]),true)],[1-leaving_probab,leaving_probab])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward function\n",
    "\n",
    "The reward function is also simple and mostly depends on the destination state $s'$. If $s'$ is a final state, it means that the user leaved after seeing the last content unit and the reward is zero, and if $s'$ is non-terminal state then the user pressed \"next\" and the reward is 1.\n",
    "The only case when the reward is positive and the visitor arrived at the final state is when she saw all the pages.\n",
    "<br> **Remark:**<br>\n",
    "In addition -100 reward was added when the page that was already seen and is proposed to the user again, it was done for out of the box POMDPs solvers, to prevent revisiting the pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.reward(mdp::ContentProducerMdp, state::State1, act::Action1, stateP::State1)\n",
    "    \n",
    "    # Collect statistics\n",
    "    RecordStatistics(mdp, state, stateP, act)\n",
    "\n",
    "    \n",
    "    are_all_pages_seen = (POMDPs.isterminal(mdp,stateP) \n",
    "                        && length(state.visited_pages) >= (length(mdp.pages) - 1))\n",
    "    \n",
    "    # Attemp to show the same page twice\n",
    "    if act.show_page in state.visited_pages\n",
    "        return -100\n",
    "    \n",
    "    # The user pressed \"next\"     \n",
    "    elseif !POMDPs.isterminal(mdp, stateP) ||  are_all_pages_seen\n",
    "        return 1\n",
    "    \n",
    "    # The user pressed \"leave\"\n",
    "    else\n",
    "        return 0\n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auxiliary functions specified in POMDPs interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the initial state and the discount factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POMDPs.discount(mdp::ContentProducerMdp) = 1\n",
    "\n",
    "POMDPs.initialstate(mdp::ContentProducerMdp)= SparseCat([State1(Tuple([]),false)],[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The POMDPs interface requires also implementing indexing function for states and actions. Here we assign a number for each state and action. State's index is actually a binary encoding of what pages has been visited. And for actions it's the page's id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "function POMDPs.stateindex(mdp::ContentProducerMdp, state::State1)\n",
    "    indeces = zeros(Int64,length(mdp.pages))\n",
    "    i = 1\n",
    "    visited_pages = state.visited_pages\n",
    "    for j in 1:length(mdp.pages)\n",
    "        if i > length(visited_pages)\n",
    "           break \n",
    "        end\n",
    "        if mdp.pages[j] == visited_pages[i]\n",
    "           \n",
    "            indeces[j] = 1\n",
    "            i+=1\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if POMDPs.isterminal(mdp,state)\n",
    "        return  BitArrToInt(ones(Int64,length(mdp.pages))) + 2\n",
    "    end\n",
    "    return BitArrToInt(indeces)+ 1\n",
    "end\n",
    "\n",
    "function POMDPs.actionindex(mdp::ContentProducerMdp,act::Action1)\n",
    "   return act.show_page.id\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Statistics collecting functions\n",
    "The following functions is used to initialize and record the statistics of particular instance of an MDP. We can analyze and visualize different aspects of experiments. For each instance of the ContentProducerMdp the statistics can be stopped, started, processed and displayed. \n",
    "\n",
    "Currently we record:\n",
    "1. The number of pages viewed by each user\n",
    "2. The number of times the user pressed \"next\" after seeing each page\n",
    "3. The number of times the user pressed \"leave\" after seeing each page\n",
    "4. Total pages seen \n",
    "5. The number of users that participated in the current experiment\n",
    "6. The number of page faults - when the same page was proposed to the user twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     8
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "function InitStatistics1(pages_number :: Int64)\n",
    "    nexts_per_page = zeros(Int64,pages_number)\n",
    "    leaves_per_page = zeros(Int64,pages_number)\n",
    "    \n",
    "    return MdpStatistics1([],nexts_per_page,leaves_per_page,0,0,[],0)\n",
    "end\n",
    "\n",
    "function RecordStatistics(mdp::ContentProducerMdp, state::State1,next_state::State1,act::Action1)\n",
    "    \n",
    "    if ! mdp.statistics_on \n",
    "       return \n",
    "    end\n",
    "    \n",
    "    statistics = mdp.statistics\n",
    "    current_page = act.show_page\n",
    "    page_id =  current_page.id  \n",
    "    \n",
    "    is_all_pages_seen = (POMDPs.isterminal(mdp,next_state) \n",
    "                        && length(state.visited_pages) >= (length(mdp.pages) - 1))\n",
    "    \n",
    "    if act.show_page in state.visited_pages\n",
    "        statistics.page_fialures_num += 1\n",
    "        statistics.users_number += 1\n",
    "        push!(statistics.pages_seen_per_user, length(statistics.current_user_path) )\n",
    "        statistics.current_user_path = Page[]\n",
    "        \n",
    "    elseif !POMDPs.isterminal(mdp,next_state) ||  is_all_pages_seen\n",
    "        push!(statistics.current_user_path, current_page)\n",
    "        statistics.nexts_per_page[page_id] +=1\n",
    "        statistics.total_pages_seen += 1\n",
    "        \n",
    "    else \n",
    "        push!(statistics.current_user_path, current_page)\n",
    "        statistics.leaves_per_page[page_id] +=1\n",
    "        statistics.users_number += 1\n",
    "        push!(statistics.pages_seen_per_user, length(statistics.current_user_path) )\n",
    "        statistics.current_user_path = Page[]\n",
    "        statistics.total_pages_seen += 1\n",
    "        \n",
    "    end\n",
    "    #println(statistics)   \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests of the MDP\n",
    "<a id='tests'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage all the required parts of POMDPs interface were implemented. Now we can start testing the model we designed. In this section we won't try to provide efficient strategies for the problem solving along with interesting users appearances. Instead we will test the implementation with simple users and out-of-the-box POMDPs solvers and simulators.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1\n",
    "<a id='test1'></a>\n",
    "&emsp;**Settings:** *10 pages, 100 similar users, random policy, leaving probabilities for pages:* $(0.1, 0.2,...,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "#### Basic objects' instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auxiliary functions \n",
    "Creating user preferences function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "function userPreferencesCreate(pages :: Array{Page,1}, page_probab :: Array{Float64,1}) \n",
    "    page_prob_dict = Dict(pages .=> page_probab)\n",
    "    \n",
    "   function userPreferences(prev_pages :: Array{Page,1}, curr_page :: Page)\n",
    "       return  page_prob_dict[curr_page]\n",
    "    end    \n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### User and Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create the pages and the user instance. There will be 10 pages $C = \\{c_1\\dots c_{10}\\}$ and a simple user with the following leaving probabilities for each page:<br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P_{leave}(c_{10})= 0.1, P_{leave}(c_9)= 0.2 \\dots P_{leave}(c_1)= 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages1 = CreatePages(10)\n",
    "\n",
    "user1 = User1(1,\n",
    "    userPreferencesCreate(pages1,collect(1:-0.1:0.1))\n",
    "             );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MDP object\n",
    "MDP and statistics initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics1 = InitStatistics1(length(pages1))\n",
    "\n",
    "mdp1 = ContentProducerMdp(pages1,user1,statistics1,true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy\n",
    "At this step we select a random policy that is provided by POMDPs package to perform an initial testing for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy1 = RandomPolicy(mdp1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simulator\n",
    "The most simple POMDPs simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RolloutSimulator();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 100 iteration of random policy execution on the MDP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in 1:100\n",
    "    r = simulate(rs,mdp1,policy1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics and plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics summary for Test 1. The third number is an objective value of the MANP problem ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "totalPagesSeen1 =mdp1.statistics.total_pages_seen\n",
    "averagePagesSeen1 = totalPagesSeen1/mdp1.statistics.users_number\n",
    "println(\"====================================================================\")\n",
    "println(\"Total number of pages seen: $(totalPagesSeen1)\")\n",
    "println(\"The number of users: $(mdp1.statistics.users_number)\")\n",
    "println(\"Average number of pages seen per user: $averagePagesSeen1\")\n",
    "println(\"====================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the distribution of the pages' number viewed by each user. <br>\n",
    "As we may see approximately half of the users proceeded to the second page, about $\\frac{1}{4}$ to the third one. There are almost no users that saw more than 6 pages when the policy is completely random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "function pages_read_dist_histogram(mdp :: ContentProducerMdp)\n",
    "    pages_num = length(mdp.pages)\n",
    "    pages_seen_per_user = mdp.statistics.pages_seen_per_user\n",
    "   pathes_length = [count((x)-> x==len,pages_seen_per_user) for len in 1:pages_num]\n",
    "    groupedbar(1:pages_num,\n",
    "            reshape(pathes_length,length(pathes_length),1),\n",
    "            bar_position = :stack,\n",
    "            bar_width=0.7,\n",
    "            xticks =(1:pages_num),\n",
    "            title=\"Distribution of number of pages seen by one user\",\n",
    "            xlabel=\"Number of Pages\",\n",
    "            leg=false,\n",
    "            framestyle=:box) \n",
    "end\n",
    "\n",
    "pages_read_dist_histogram(mdp1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at the distribution of the success/failure (next/leave) and the success ratio for each page.<br>\n",
    "At the first histogram we present \"next\"s and \"leave\"s number in one bar for each one of 10 pages. <br>\n",
    "At the second one we see the ration of the users that pressed \"next\" for each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "function action_distribution_hist(mdp:: ContentProducerMdp)\n",
    "    \n",
    "    pages_num = length(mdp.pages)\n",
    "    stat = mdp.statistics\n",
    "    \n",
    "    \n",
    "    hist_data = zeros(pages_num, 3)\n",
    "    success_rate = zeros(pages_num)\n",
    "\n",
    "pages_num = length(mdp.pages)\n",
    "    \n",
    "for page in mdp.pages\n",
    "    nexts = stat.nexts_per_page[page.id]\n",
    "    leaves = stat.leaves_per_page[page.id]\n",
    "    total = nexts + leaves \n",
    "    hist_data[page.id,1] = nexts\n",
    "    hist_data[page.id,2] = leaves\n",
    "\n",
    "    if total > 0\n",
    "        success_rate[page.id] =  nexts / total \n",
    "    end\n",
    "end\n",
    "\n",
    "g= groupedbar(hist_data,bar_position = :stack,\n",
    "                    xlabel=\"Page ID Number\",\n",
    "                    ylabel=\"Number of Actions\",\n",
    "                    xticks =(1:pages_num),\n",
    "                    title=\"Number of \\\"next\\\" and \\\"leave\\\" actions  per page\",\n",
    "                    framestyle=:box,\n",
    "                    group=repeat([\"Pressed Next\",\"Leaved\"],inner=pages_num))\n",
    "\n",
    "\n",
    "g1 = groupedbar(reshape(success_rate,length(success_rate),1),\n",
    "                    \n",
    "                    xticks =(1:pages_num),\n",
    "                    xlabel=\"Page ID Number\",\n",
    "                    ylabel=\" Next actions/Total \\n ratio\",\n",
    "                    title=\"Success ratio: the fraction of cases when \\n the agent pressed next per page\",\n",
    "                    framestyle=:box,\n",
    "                    leg=false)\n",
    "\n",
    "\n",
    "plot(g,g1,size=(800,800),layout=(2,1))\n",
    "    \n",
    "end\n",
    "\n",
    "action_distribution_hist(mdp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 \n",
    "<a id='test2'></a>\n",
    "**Settings:** *10 pages, 100 same users, value iteration optimized policy, leaving probabilities for pages:*  $(0.1,0.2,...,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same user and the same pages as in Test 1 , the only thing we will change is a policy. \n",
    "\n",
    "As in the previous section, there will be 10 pages, with leaving probabilities from 0.1 to 1 with step of 0.1. But now we will evaluate the states with value iteration, and then we will use greedy policy to run the experiment. \n",
    "\n",
    "In fact we know that the resulting policy should actually \"arrange the pages by their leaving probabilities\". The first action will show the page with the lowest leaving probability and the last one will be the page with the highest probability. It is actually the best policy possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The new MDP object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics2 = InitStatistics1(length(pages1))\n",
    "mdp2 = ContentProducerMdp(pages1,user1,statistics2,false); # At the stage of value iteration the statistics collecting \n",
    "                                                          # will be off (the last argument)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value iteration and optimal policy production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the solver\n",
    "# belres: the value of Bellman residual used in the solver (defualt is 1e-3)\n",
    "solver2 = ValueIterationSolver(max_iterations=100, belres=1e-3; verbose=true)\n",
    "\n",
    "# solve for an optimal policy\n",
    "policy2 = solve(solver2, mdp2);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simulation\n",
    "Now lets simulate 100 users arrivals and see how the results have changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp2.statistics_on = true\n",
    "\n",
    "rs2 = RolloutSimulator()\n",
    "for _ in 1:100\n",
    "    r = simulate(rs,mdp2,policy2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "totalPagesSeen2 =mdp2.statistics.total_pages_seen\n",
    "averagePagesSeen2 = totalPagesSeen2/mdp2.statistics.users_number\n",
    "println(\"====================================================================\")\n",
    "println(\"Total number of pages seen: $(totalPagesSeen2)\")\n",
    "println(\"The number of users: $(mdp2.statistics.users_number)\")\n",
    "println(\"Average number of pages seen per user: $averagePagesSeen2\")\n",
    "println(\"====================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average number of pages viewed raised to more than 3.5 from about 1.6 in the random policy experiment. The distribution of the number of pages viewed looks completely different. Almost $1/4$ of the users arrived at more then 6 pages. The largest amount of people viewed 5 and 6 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_read_dist_histogram(mdp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Temporary conclusion\n",
    "<a id='conclude'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "MAX_USERS = 50\n",
    "pages_random = statistics1.pages_seen_per_user[1:MAX_USERS]\n",
    "pages_best = statistics2.pages_seen_per_user[1:MAX_USERS]\n",
    "\n",
    "pl = plot(1:MAX_USERS, pages_random,label=\"Random policy\",xlabel=\"User's id\",\n",
    "                    ylabel=\"Number of pages seen\",\n",
    "                    title=\"Best and Random policy: number of pages seen per user\",\n",
    "                    framestyle=:box,)\n",
    "plot!(pl,1:MAX_USERS, pages_best,label=\"Best policy\")\n",
    "hline!(pl,[averagePagesSeen1],label=\"Random policy average\")\n",
    "hline!(pl,[averagePagesSeen2],label=\"Best policy average\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We showed the behavior of two different policies in our MDP - the best and a random policy. It will be the benchmarks in our future work trying to find the best policy and trying to learn this policy in an optimal way. The goal remains the same: organizing the learning process such that the overall performance (Average number of pages read in multi-page publications) will be as good as possible for variable number of users and pages. The main fundamental issue here will be balancing exploration and exploitation. And the major measure of success will be the regret.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TiuTa / Chernovik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition probabilities are known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine the situation when the preferences of the visitor are known, and they are encapsulated in the probability of leaving for every particular page. It completely defines the transition function and the MDP . In this case the  problem is reduced to the planning problem.<br> \n",
    "These four characteristics made the current planning problem's solution trivial.\n",
    "1. The transition function is independent of the pages the user has visited, and depend only on the current page shown. \n",
    "2. The states can't be repeated for each particular user, the user can see each page only once.\n",
    "3. The user can traverse to exactly two states given some action,and one of them is $s_{final}$.\n",
    "4. The reward depends only on the forthcoming state, and it is equal for all states except for the final.\n",
    "\n",
    "Let's consider the state value function given any policy $\\pi$.<br>\n",
    "&emsp;$V(s_i) = \\sum_{j}T(\\pi(s_i),s_i,s_j)( R(\\pi(s_i),s_i,s_j) + V(s_j) )$<br>\n",
    "There is only one possible descending state with non-zero value for each action (The provider presents content to the visitor and she proceeds to the next page) and the reward is 1. As a result we can simplify the value function for our MDP:<br>\n",
    "&emsp;$V(s_i) = T(\\pi(s_i),s_i,s_j)( 1 + V(s_j) )$ ,<br> If we substitute the previous expression for each state into the value function of the initial state, we receive <br> \n",
    "&emsp;$V(s_{init}) =  T(\\pi(s_{init}),s_{init},s_i)( 1 +  T(\\pi(s_i),s_i,s_j)( 1 + \\dots ) )$.<br>\n",
    "\n",
    "Given the fact that: (1) each state can appear only once, (2) the transition probability depends only on the action, we can say that in order to maximize the value of $s_{init}$ (and as a result find an optimal policy) we need to arrange an actions in descending order by there transition probabilities. That is the optimal policy's action in the initial state will be $\\arg\\max_{\\pi(s_{init})\\in A} T(\\pi(s_{init}),s_{init},s_i)$ and so on.<br>\n",
    "<br>\n",
    "As we said before, the planning problem turns to be trivial in case we know all the data.<br>\n",
    "Now let's consider the case when our simplifying assumptions remain valid, but we don't know user's preferences.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition probabilities are unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknown content consumer's preferences mean that we don't know the exact probability of \"leaving\" for each particular page $c_i \\in B$. In terms of the MDP it means that the transition function is unknown. But in fact it is only partially true. Given an action $a_{c_i}$ (showing to the user the page $c_i$), we know exactly the states that the process has non-zero probabilities to traverse to. This is the final state and the state that contains all the pages the user visited including the last one $c_i$. So actually we need to find only the probability of leaving after seeing each page and the MDP will be fully known.<br>\n",
    "This problem turns to be a learning problem.<br>\n",
    "Again, the problem's structure together with the assumptions makes it relatively simple learning problem. Despite the fact that there are $2^{|B|} + 2$ states, only actions have an impact on the transition probability. Thus we must estimate only $|B|$ values: the probabilities of leaving for each page. <br>\n",
    "\n",
    "The simple \"model learning -> planning\" loop seems appropriate solution to deal it. But even though each step appears to be non-complicated, combining them together arise a fundamental exploration/exploitation dilemma.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
