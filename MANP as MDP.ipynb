{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MANP problem (*M*aximizing *A*verage *N*umber of *P*ages read in multi-page publications) as an MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will present MANP problem as MDP. In order to simplify the representation at this stage we add two constraints on the initial model:\n",
    "1. On each step the agent has only two possible actions - \"next\" and \"leave\".\n",
    "2. The agent can't visit the same page twice.\n",
    "\n",
    "The problem remains the same: given a set of content units (pages)  $B = \\{c_1\\dots c_K\\}$, we want to present it to users in such a way that will maximize the average number of pages viewed. The only thing we can change is an order of the representation for each particular user.<br>\n",
    "\n",
    "Even though the naive and straightforward way to define the MDP is to set the content consumer being an agent, we will do it in a different way. The content producer will be an agent and the Web resource visitor will be the part of the environment. Such a formulation allows us to use classic tools of planning to optimize the target MANP problem's value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr style=\"border:2px solid gray\"> </hr>\n",
    "\n",
    "**Here is a definition of the MDP:**<br>\n",
    "\n",
    " 1. State space $S = S' \\cup \\{s_{init},s_{final}\\}$ : \n",
    "    * There will be an initial state $s_{init}$. The visitor haven't seen yet any content.\n",
    "    * Each state $s_j = C_k \\in S'$ will contain all the pages the user saw $C_k \\subset B$ (nodes the user visited).\n",
    "    * There will be a final state $s_{final}$. It represents the fact that the user finished watching the pages.\n",
    " <br> <br> \n",
    " 2.  Action space $A=\\{a_{c_1}\\dots a_{c_k}\\}$. Action $a_{c_k}$ denotes the action of presenting  to the visitor page $c_k \\in B$.\n",
    " <br><br>\n",
    " 3.  Reward function $R(a,s,s')$ for $a \\in A\\;s,s'\\in S$  depends only on the resulting state $s'$ :\n",
    "     * If the agent traversed to $s_{final}$, the reward is zero. Otherwise it's 1.<br>\n",
    "        $\\forall a\\in A\\;s_i\\in S :R(a,s_i,s_{final}) = 0$, otherwise $R(a,s,s')=1$\n",
    " <br><br>\n",
    " 4. Transition function $T(a,s,s')$ depends only on action $a$ and it is not known to us. We will denote it as $T'(a) = T(a,s,s')$<br>\n",
    "   It actually determines possible reactions of the content consumer to the page $c_i$ (or for action $a_{c_i}$ in terms of MDP) , the probability that the user press \"next\" or \"leave\". <br>\n",
    "   At the next stages we may assume that it depends also on $s$ ,i.e. visitors behavior depends on the pages seen.\n",
    "<hr style=\"border:2px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='background:#D3D3D3'> POMDPs.jl </span> package will be used to help modeling the MDP. It provides flexible interface that is compatable with different tools for solving and simulating an MDP. It specifies the set of functions that should be implemented. In fact we overload the methods of POMDPs package to work with the process we define. <<< link to the page of mdp>>>\n",
    "\n",
    "In following several subsections we implement this methods one by one and provide a short explanation for the  implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try\n",
    "    using Pkg\n",
    "    using POMDPs            # POMDPs.jl interface\n",
    "    using POMDPModelTools   # POMDPModelTools has tools that help build the MDP definition\n",
    "    using POMDPPolicies\n",
    "    using POMDPSimulators\n",
    "    using Combinatorics\n",
    "    using DiscreteValueIteration\n",
    "catch  LoadError\n",
    "    Pkg.add(\"POMDPs\")\n",
    "    Pkg.add(\"POMDPModelTools\")\n",
    "    Pkg.add(\"POMDPPolicies\")\n",
    "    Pkg.add(\"POMDPSimulators\")\n",
    "    Pkg.add(\"Combinatorics\")\n",
    "    Pkg.add(\"DiscreteValueIteration\")\n",
    "    using DiscreteValueIteration\n",
    "    using POMDPs\n",
    "    using POMDPModelTools\n",
    "    using Combinatorics\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic objects\n",
    "First two basic objects for the model are **Page** and **User1**. \n",
    "\n",
    " <span style='background:#D3D3D3'> User1 </span> object will encapsulate the particular user's behavior.\n",
    "Although the user is a part of an environment in this model, and it may be fully described by the transition function of the process , we decided to represent it as a separate object. It was done for the several reasons: 1) to preserve the initial structure of MANP problem , where the user is an essential part  2) to have a convenient ability to change the user's behavior i.e simulating the arrival of different types of users 3) reusing the same object in other models for MANP problem.\n",
    "\n",
    "Page is the object that represents the unit of content which the user consumes (watches) before deciding on her next action (leave or proceed) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Page\n",
    "   id :: Int64 \n",
    "end\n",
    "\n",
    "pages_collection = nothing\n",
    "\n",
    "struct User1 \n",
    "   id :: Int64\n",
    "   userPreferencesFunction:: Any   # Recieve pages visited, current page shown\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     1,
     5
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "function CreatePages(number :: Int64)\n",
    "    global pages_collection = [Page(id) for id in 1:number]\n",
    "end\n",
    "\n",
    "function bitarr_to_int(arr)\n",
    "    return sum(arr .* (2 .^ collect(0:length(arr)-1)))\n",
    "    end;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP basic elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we define the parts of the MDP: actions , states and the mdp object itself.\n",
    "We remind that our representation, when the user is part of the environment and the content provider is an agent, may seem counterintuitive at first sight. But it makes sense supposing that the goal is to maximize the number of pages viewed, which is a content provider's goal.\n",
    "\n",
    "The <span style='background:#D3D3D3'> Action1</span>  object represents the action of presenting to the user a particular unit of content (aka page) .  \n",
    "The <span style='background:#D3D3D3'> State1</span> object specifies state that contains the pages that the user already have seen and the indicator whether it is a terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Action1\n",
    "   show_page :: Union{Page,Nothing}           # The page that is presented to the user\n",
    "end\n",
    "\n",
    "struct State1\n",
    "    visited_pages :: Any                      # Didn't specified, because we need here immutable type.                                              # Supposed to be an immutable set\n",
    "    if_terminal :: Bool\n",
    "end "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDP object\n",
    "\n",
    "As a part of the POMDPs interface specifications , we should define an MDP object that inherits from the abstract type MDP and specify the type of the actions' and states' object. The structure <span style='background:#D3D3D3'> ContentProducerMdp </span> contains the parameters of the MDP environment. It is passed as a first argument to all the POMDPs interface's functions to navigate to the implementations that was written for this concrete MDP. \n",
    "\n",
    "It will contain a current user that determine the process dynamics, array of content units (pages) as well as tools for tracking and statistics.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "struct MdpStatistics1\n",
    "   current_user_path :: Array{Page,1} \n",
    "   nexts_per_page :: Array{Int64,1}\n",
    "    leaves_per_page :: Array{Int64,1}\n",
    "    total_pages_seen :: Int64\n",
    "    users_number :: Int64\n",
    "    \n",
    "end\n",
    "\n",
    "mutable struct ContentProducerMdp <: MDP{State1,Action1}\n",
    "    pages :: Array{Page,1}\n",
    "    current_user :: User1\n",
    "    statistics :: MdpStatistics1\n",
    "    \n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auxiliary functions\n",
    "Return the array of all the state and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0,
     9
    ]
   },
   "outputs": [],
   "source": [
    "function POMDPs.states(mdp::ContentProducerMdp)\n",
    "    pages_superset = collect(powerset(mdp.pages))             # We assume that the function preserves the order (not in docs)\n",
    "    states = [State1(Tuple(pages),false) for pages in pages_superset]  \n",
    "    push!(states, State1(Tuple([]),true))\n",
    "    return states\n",
    "end\n",
    "\n",
    "POMDPs.isterminal(mdp::ContentProducerMdp,s::State1)= return s.if_terminal\n",
    "\n",
    "function POMDPs.actions(mdp::ContentProducerMdp)\n",
    "    actions = [Action1(page) for page in mdp.pages]  \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition function\n",
    "\n",
    "Transition function returns distribution on states. In current less complicated formulation of the problem, the transition distribution depends only on the action. The user decision whether to proceed or to leave explained only by page that was presented him last.\n",
    "\n",
    "Exactly two states have non-zero probabilities given action $a_i$. They are 1) the final state and 2) the state that contains all the pages the user visited including the last one $c_i$. The distribution on these states is defined by user's leaving probability for each page. The transition function queries the current user (userPreferencesFunction attribute of User1) , that appears in the MDP object for it. <br>\n",
    "Special case is when the visitor already saw all the pages, and it is automatically traversed to the final state. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.transition(mdp::ContentProducerMdp,state::State1,act::Action1)\n",
    "\n",
    "    pages_visited = state.visited_pages\n",
    "    current_page = act.show_page\n",
    "    \n",
    "    if current_page in pages_visited\n",
    "        return SparseCat([State1(Tuple([]),true)], [1.0]) \n",
    "    end\n",
    "    \n",
    "    if POMDPs.isterminal(mdp,state)\n",
    "       return SparseCat([State1(Tuple([]),true)], [1.0]) \n",
    "    end\n",
    "    \n",
    "    if length(pages_visited) == length(mdp.pages) - 1\n",
    "       return SparseCat([State1(Tuple([]),true)], [1.0])\n",
    "    end\n",
    "\n",
    "    leaving_probab = mdp.current_user.userPreferencesFunction(current_page,pages_visited)\n",
    "    \n",
    "       if length(pages_visited) == 0\n",
    "        new_page_visited = [current_page]\n",
    "    else\n",
    "\n",
    "        new_page_visited = push!(collect(pages_visited),current_page)\n",
    "    end\n",
    "    sort!(new_page_visited,by=page->page.id,alg=InsertionSort)  # The array is almost sorted , that's why insertion sort\n",
    "    \n",
    "    return SparseCat([State1(Tuple(new_page_visited),false), State1(Tuple([]),true)],[1-leaving_probab,leaving_probab])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward function\n",
    "\n",
    "The reward function is also simple and mostly depends on the destination state $s'$. If $s'$ is a final state, it means that the user leaved after seeing the last content unit and the reward is zero, and if $s'$ is non-terminal state then the user pressed \"next\" and the reward is 1.\n",
    "The only case when the reward is positive when the visitor arrived at the final state is when she saw all the pages.\n",
    "Remark:\n",
    "In addition -100 reward was added when the page that was already seen in proposed to the user again, it was done for out of the box POMDPs solvers, to prevent revisiting the pages.\n",
    "\n",
    "\n",
    "The reward is in fact a number of pages that the visitor saw before leaving, that is exactly the MANP objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.reward(mdp::ContentProducerMdp,state::State1,act::Action1,stateP::State1)\n",
    "    is_all_pages_seen = (POMDPs.isterminal(mdp,stateP) \n",
    "                        && length(state.visited_pages) >= (length(mdp.pages) - 1))\n",
    "    \n",
    "    if act.show_page in state.visited_pages\n",
    "        return -100\n",
    "    elseif !POMDPs.isterminal(mdp,stateP) ||  is_all_pages_seen\n",
    "        return 1\n",
    "    else \n",
    "        return 0\n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the initial state and the discount factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "POMDPs.discount(mdp::ContentProducerMdp) = 1\n",
    "\n",
    "POMDPs.initialstate(mdp::ContentProducerMdp)= SparseCat([State1(Tuple([]),false)],[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The POMDPs interface requires also implementing indexing function for states and actions. Here we assign a number for each state and action. State's index is actually a binary encoding of what pages has been visited. And for actions it's the page id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     0,
     21
    ]
   },
   "outputs": [],
   "source": [
    "function POMDPs.stateindex(mdp::ContentProducerMdp, state::State1)\n",
    "    indeces = zeros(Int64,length(mdp.pages))\n",
    "    i = 1\n",
    "    visited_pages = state.visited_pages\n",
    "    for j in 1:length(mdp.pages)\n",
    "        if i > length(visited_pages)\n",
    "           break \n",
    "        end\n",
    "        if mdp.pages[j] == visited_pages[i]\n",
    "           \n",
    "            indeces[j] = 1\n",
    "            i+=1\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if POMDPs.isterminal(mdp,state)\n",
    "        return  bitarr_to_int(ones(Int64,length(mdp.pages))) + 2\n",
    "    end\n",
    "    return bitarr_to_int(indeces)+ 1\n",
    "end\n",
    "\n",
    "function POMDPs.actionindex(mdp::ContentProducerMdp,act::Action1)\n",
    "   return act.show_page.id\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following sections are not yet completed \n",
    "### Tests of policy run and mdp solving +  mdp  statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics (*Under construction*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecordStatistics (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "function InitStatistics1(pages_number :: Int64)\n",
    "    nexts_per_page = zeros(pages_number)\n",
    "    leaves_per_page = zeros(pages_number)\n",
    "    \n",
    "    return MdpStatistics1([],nexts_per_page,leaves_per_page,0,0)\n",
    "end\n",
    "\n",
    "function RecordStatistics(mdp::ContentProducerMdp, prev_state::State1,next_state::State1,act::Action1)\n",
    "    statistics = mdp.statistics\n",
    "    current_page = act.show_page\n",
    "    \n",
    "    is_all_pages_seen = (POMDPs.isterminal(mdp,stateP) \n",
    "                        && length(state.visited_pages) >= (length(mdp.pages) - 1))\n",
    "    \n",
    "    if act.show_page in state.visited_pages\n",
    "        push!(statistics.current_user_path, current_page)\n",
    "        push!(statistics.nexts_per_us)\n",
    "    elseif !POMDPs.isterminal(mdp,stateP) ||  is_all_pages_seen\n",
    "        push!(statistics.current_user_path, current_page)\n",
    "    else \n",
    "        push!(statistics.current_user_path, current_page)\n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics = InitStatistics1(5)\n",
    "\n",
    "mdp = ContentProducerMdp(CreatePages(5),User1(1,(x,y)->0.1),statistics)\n",
    "\n",
    "##\n",
    "    state = State1(Tuple([]),false)\n",
    "\n",
    "    action1 = Action1(Page(4))\n",
    "    # POMDPs.states(mdp)\n",
    "    # POMDPs.actions(mdp)\n",
    "\n",
    "    POMDPs.transition(mdp,state,action1)\n",
    "\n",
    "    POMDPs.reward(mdp,state,action1,state)#State1(Tuple([]),true))\n",
    "    # POMDPs.stateindex(mdp,state)\n",
    "\n",
    "    # POMDPs.actionindex(mdp,action1)\n",
    "\n",
    "    # p = Page(1)\n",
    "\n",
    "    # State1((Page(2),),true) == State1((Page(1),),true)\n",
    "\n",
    "    # collect(powerset([:a,1,\"dsa\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random policy + step by step run test (*Under construction*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s = State1((), false)\n",
      "a = Action1(Page(1))\n",
      "r = 1\n",
      "\n",
      "s = State1((Page(1),), false)\n",
      "a = Action1(Page(1))\n",
      "r = -100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "policy = RandomPolicy(mdp)\n",
    "\n",
    "for (s,a,r) in stepthrough(mdp, policy, \"s,a,r\", max_steps=10)\n",
    "    @show s\n",
    "    @show a\n",
    "    @show r\n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value iteration solver test (*Under construction*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 1   ] residual:        100 | iteration runtime:      0.719 ms, (  0.000719 s total)\n",
      "[Iteration 2   ] residual:        0.9 | iteration runtime:      2.106 ms, (   0.00282 s total)\n",
      "[Iteration 3   ] residual:       0.81 | iteration runtime:      0.644 ms, (   0.00347 s total)\n",
      "[Iteration 4   ] residual:      0.729 | iteration runtime:      0.658 ms, (   0.00413 s total)\n",
      "[Iteration 5   ] residual:      0.656 | iteration runtime:      3.286 ms, (   0.00741 s total)\n",
      "[Iteration 6   ] residual:          0 | iteration runtime:      0.878 ms, (   0.00829 s total)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "POMDPs.isterminal(s::State1)=POMDPs.isterminal(mdp,s)\n",
    "# initialize the problem\n",
    "\n",
    "# initialize the solver\n",
    "# max_iterations: maximum number of iterations value iteration runs for (default is 100)\n",
    "# belres: the value of Bellman residual used in the solver (defualt is 1e-3)\n",
    "solver = ValueIterationSolver(max_iterations=10, belres=1e-3; verbose=true)\n",
    "\n",
    "# solve for an optimal policy\n",
    "policy = solve(solver, mdp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Action1(Page(2))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = action(policy, State1((Page(1),), false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition probabilities are known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine the situation when the preferences of the visitor are known, and they are encapsulated in the probability of leaving for every particular page. It completely defines the transition function and the MDP . In this case the  problem is reduced to the planning problem.<br> \n",
    "These four characteristics made the current planning problem's solution trivial.\n",
    "1. The transition function is independent of the pages the user has visited, and depend only on the current page shown. \n",
    "2. The states can't be repeated for each particular user, the user can see each page only once.\n",
    "3. The user can traverse to exactly two states given some action,and one of them is $s_{final}$.\n",
    "4. The reward depends only on the forthcoming state, and it is equal for all states except for the final.\n",
    "\n",
    "Let's consider the state value function given any policy $\\pi$.<br>\n",
    "&emsp;$V(s_i) = \\sum_{j}T(\\pi(s_i),s_i,s_j)( R(\\pi(s_i),s_i,s_j) + V(s_j) )$<br>\n",
    "There is only one possible descending state with non-zero value for each action (The provider presents content to the visitor and she proceeds to the next page) and the reward is 1. As a result we can simplify the value function for our MDP:<br>\n",
    "&emsp;$V(s_i) = T(\\pi(s_i),s_i,s_j)( 1 + V(s_j) )$ ,<br> If we substitute the previous expression for each state into the value function of the initial state, we receive <br> \n",
    "&emsp;$V(s_{init}) =  T(\\pi(s_{init}),s_{init},s_i)( 1 +  T(\\pi(s_i),s_i,s_j)( 1 + \\dots ) )$.<br>\n",
    "\n",
    "Given the fact that: (1) each state can appear only once, (2) the transition probability depends only on the action, we can say that in order to maximize the value of $s_{init}$ (and as a result find an optimal policy) we need to arrange an actions in descending order by there transition probabilities. That is the optimal policy's action in the initial state will be $\\arg\\max_{\\pi(s_{init})\\in A} T(\\pi(s_{init}),s_{init},s_i)$ and so on.<br>\n",
    "<br>\n",
    "As we said before, the planning problem turns to be trivial in case we know all the data.<br>\n",
    "Now let's consider the case when our simplifying assumptions remain valid, but we don't know user's preferences.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition probabilities are unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknown content consumer's preferences mean that we don't know the exact probability of \"leaving\" for each particular page $c_i \\in B$. In terms of the MDP it means that the transition function is unknown. But in fact it is only partially true. Given an action $a_{c_i}$ (showing to the user the page $c_i$), we know exactly the states that the process has non-zero probabilities to traverse to. This is the final state and the state that contains all the pages the user visited including the last one $c_i$. So actually we need to find only the probability of leaving after seeing each page and the MDP will be fully known.<br>\n",
    "This problem turns to be a learning problem.<br>\n",
    "Again, the problem's structure together with the assumptions makes it relatively simple learning problem. Despite the fact that there are $2^{|B|} + 2$ states, only actions have an impact on the transition probability. Thus we must estimate only $|B|$ values: the probabilities of leaving for each page. <br>\n",
    "\n",
    "The simple \"model learning -> planning\" loop seems appropriate solution to deal it. But even though each step appears to be non-complicated, combining them together arise a fundamental exploration/exploitation dilemma.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
